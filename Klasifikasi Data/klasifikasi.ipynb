{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27e9b422",
   "metadata": {},
   "source": [
    "# KLASIFIKASI DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd673a4",
   "metadata": {},
   "source": [
    "## IMPORT LIBRARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78e7dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import string\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fba541ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\62877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\62877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\62877\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7688741e",
   "metadata": {},
   "source": [
    "## LOAD DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "f180641b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kolom dataset: Index(['clean_text', 'sentimen'], dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>clean_text</th>\n",
       "      <th>sentimen</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>yes betul sebagai pelatih timnas menangani tim...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>gua gak peduli siapa yang main yang penting ti...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>peluang timnas indonesia ke piala dunia masih ...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>berharap menang dari jepang dianggap gak masuk...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>iya iya gak boleh ngarep menang dari jepang ga...</td>\n",
       "      <td>positif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                          clean_text sentimen\n",
       "0  yes betul sebagai pelatih timnas menangani tim...  positif\n",
       "1  gua gak peduli siapa yang main yang penting ti...  positif\n",
       "2  peluang timnas indonesia ke piala dunia masih ...  positif\n",
       "3  berharap menang dari jepang dianggap gak masuk...  positif\n",
       "4  iya iya gak boleh ngarep menang dari jepang ga...  positif"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('data_label_manual.csv')\n",
    "\n",
    "print(\"Kolom dataset:\", df.columns)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33bba29",
   "metadata": {},
   "source": [
    "## PREPROCESSING (Stemming & Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d779f287",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('indonesian'))\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    text = text.strip()\n",
    "\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in stop_words]\n",
    "\n",
    "    tokens = [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "    tokens = [lemmatizer.lemmatize(t) for t in tokens]\n",
    "\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "df['clean_text'] = df.iloc[:, 0].apply(preprocess_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b79b574",
   "metadata": {},
   "source": [
    "## SPLIT DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "577b26a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df['clean_text']\n",
    "y = df.iloc[:, 1]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54f53935",
   "metadata": {},
   "source": [
    "## EKSTRASI FITUR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89ab7bd4",
   "metadata": {},
   "source": [
    "### BOW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cfda0c17",
   "metadata": {},
   "outputs": [],
   "source": [
    "bow = CountVectorizer(max_features=5000)\n",
    "X_train_bow = bow.fit_transform(X_train)\n",
    "X_test_bow = bow.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8005c337",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "632e1fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features=5000)\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)\n",
    "X_test_tfidf = tfidf.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc15e51",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d51ac893",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [text.split() for text in X_train]\n",
    "\n",
    "w2v = Word2Vec(\n",
    "    sentences,\n",
    "    vector_size=100,\n",
    "    window=5,\n",
    "    min_count=2,\n",
    "    workers=4\n",
    ")\n",
    "\n",
    "def w2v_vector(text):\n",
    "    vectors = [w2v.wv[word] for word in text.split() if word in w2v.wv]\n",
    "    if len(vectors) == 0:\n",
    "        return np.zeros(100)\n",
    "    return np.mean(vectors, axis=0)\n",
    "\n",
    "X_train_w2v = np.array([w2v_vector(text) for text in X_train])\n",
    "X_test_w2v = np.array([w2v_vector(text) for text in X_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07b6e776",
   "metadata": {},
   "source": [
    "## KLASIFIKASI"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa37fba",
   "metadata": {},
   "source": [
    "### NAIVE BAES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "28803d41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes menggunakan BoW\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.62      0.80      0.70        20\n",
      "      netral       1.00      0.40      0.57        10\n",
      "     positif       0.70      0.70      0.70        20\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.77      0.63      0.66        50\n",
      "weighted avg       0.73      0.68      0.67        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb = MultinomialNB()\n",
    "\n",
    "nb.fit(X_train_bow, y_train)\n",
    "pred_nb_bow = nb.predict(X_test_bow)\n",
    "\n",
    "print(\"Naive Bayes menggunakan BoW\")\n",
    "print(classification_report(y_test, pred_nb_bow))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9eb301f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes menggunakan TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.58      0.75      0.65        20\n",
      "      netral       1.00      0.30      0.46        10\n",
      "     positif       0.62      0.65      0.63        20\n",
      "\n",
      "    accuracy                           0.62        50\n",
      "   macro avg       0.73      0.57      0.58        50\n",
      "weighted avg       0.68      0.62      0.61        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "nb.fit(X_train_tfidf, y_train)\n",
    "pred_nb_tfidf = nb.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Naive Bayes menggunakan TF-IDF\")\n",
    "print(classification_report(y_test, pred_nb_tfidf))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbff4fc",
   "metadata": {},
   "source": [
    "### DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "53303d25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decision Tree menggunakan TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.79      0.75      0.77        20\n",
      "      netral       0.53      0.80      0.64        10\n",
      "     positif       0.69      0.55      0.61        20\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.67      0.70      0.67        50\n",
      "weighted avg       0.70      0.68      0.68        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dt = DecisionTreeClassifier(random_state=42)\n",
    "dt.fit(X_train_tfidf, y_train)\n",
    "pred_dt = dt.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Decision Tree menggunakan TF-IDF\")\n",
    "print(classification_report(y_test, pred_dt))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "beb8e429",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "8595ad7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM menggunakan TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.65      0.75      0.70        20\n",
      "      netral       1.00      0.40      0.57        10\n",
      "     positif       0.65      0.75      0.70        20\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.77      0.63      0.66        50\n",
      "weighted avg       0.72      0.68      0.67        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm = SVC(kernel='linear')\n",
    "svm.fit(X_train_tfidf, y_train)\n",
    "pred_svm = svm.predict(X_test_tfidf)\n",
    "\n",
    "print(\"SVM menggunakan TF-IDF\")\n",
    "print(classification_report(y_test, pred_svm))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa775aa",
   "metadata": {},
   "source": [
    "### LOGISTIC REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "aa22e6f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression menggunakan TF-IDF\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.62      0.75      0.68        20\n",
      "      netral       1.00      0.40      0.57        10\n",
      "     positif       0.68      0.75      0.71        20\n",
      "\n",
      "    accuracy                           0.68        50\n",
      "   macro avg       0.77      0.63      0.66        50\n",
      "weighted avg       0.72      0.68      0.67        50\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(max_iter=1000)\n",
    "lr.fit(X_train_tfidf, y_train)\n",
    "pred_lr = lr.predict(X_test_tfidf)\n",
    "\n",
    "print(\"Logistic Regression menggunakan TF-IDF\")\n",
    "print(classification_report(y_test, pred_lr))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb74ca49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression menggunakan Word2Vec\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "     negatif       0.46      0.65      0.54        20\n",
      "      netral       0.00      0.00      0.00        10\n",
      "     positif       0.36      0.40      0.38        20\n",
      "\n",
      "    accuracy                           0.42        50\n",
      "   macro avg       0.28      0.35      0.31        50\n",
      "weighted avg       0.33      0.42      0.37        50\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\62877\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\62877\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "c:\\Users\\62877\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1318: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "lr_w2v = LogisticRegression(max_iter=1000)\n",
    "lr_w2v.fit(X_train_w2v, y_train)\n",
    "pred_w2v = lr_w2v.predict(X_test_w2v)\n",
    "\n",
    "print(\"Logistic Regression menggunakan Word2Vec\")\n",
    "print(classification_report(y_test, pred_w2v))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
